I started working on the redesigning parts of the LLVM pass manager. I'm sorry if I'm a little bit quiet and hard to hear today. It turns out this time slide is cursed. So this is part two of a kind of two-part talk. Um, I intended to give this talk in April at your LLVM. I wanted to tell people about from work had been doing on the pass manager. And I realize that we need to kind of go over the background for how the past manager help, as is an LLVM or even designed, how we think about them, what the sort of essential components are. And so I ended up talking a lot about that instead. But I still really want to talk to you guys about the new design that I've been working on for the past manager. Um, I think it sells a lot of the problems that we have today. I think it's pretty promising going forward. Um, and so I'm going to spend most of this talk talking about the core of the new passed manager design. It's going to be very low level and have tons of code up on the slide decks. Hopefully, that works for you guys. Um, I can't point at the code that's tried to highlight the pieces of the code that I want you guys to look at, and we're going to kind of step through each of the parts of the code. One of the big complaints about the previous past manager is that almost no one understood how it worked. Like, how many people here feel like they have a good handle on how the old past manager works? There will be a quiz if you want to step up to this, but it's confusing. And so my goal is that by the end of this talk, at least that isn't true for the new design. And you guys can let me know how I do. 

Okay, so from the previous talk, you don't need to have all the background to the last talk. Uh, but from there, there's a couple of critical things. Right? A pass on LLVM is kind of world is something which operates on some unit of I are uh units kind of fuzzy little find. You can have a function, you can have a module. Um, all kinds of ways we can think about how to break up the IR but you've got some chunk of it. You're going to transform it from one thing into another thing, probably equivalent for some definition of equivalence. Right? Um. Alternative use of pass million is to try and analyze that chunk of I r and drive some kind of higher-order information, right? Some higher level information about exactly what that I r does. So so I started from just this kind of very basic ideas about what a pass looks like. And I tried to see if we could actually design the past management layer around this kind of simplistic view and still have it. So all the needs and, uh, you know, like what if we actually just let the code model this very basic idea? Maybe it won't work, but it might give us a better starting point. 

And so I started with this idea of a pass. Super, super simple, right? Like you have a class, it has a constructor, the constructor sets things up. It has a run method, the run method, except some unit of I r in this case, a function. Right? And it does something with that function. Couldn't get much simpler than this. The question is how close to this could we keep everything and address the use cases we have for the past manager? Well, let's start a kind of experiment again. This is almost literally what a pass looked like on the first iteration of the new pass minute when I first checked. And if you go back through the commit history, you'll find a test case looks almost exactly like this code, right?

And so we're like, okay, well, then if that's the most straightforward idea of a pass, what-what is the simplicity of the past manager look like? How do we kind of, you know, take that down to its core principles?
Well, it does aggregation. That's the core idea of past managers. It takes a bunch of other passes, aggregates them together into some sequence, right? Runs the whole sequence over a particular unit of I r one of the key ideas of a past manager is that a past manager is itself a pass.

Right? And so you can see down here, this is a class, and it has a run method which accepts a unit of I r right? So it satisfies our definition of a pass. But it also has this like really fancy clever template metaprogramming blah that you guys probably think it's ridiculous up at the that's taking care of aggravating passes. All right? And so I think we at least need to understand how all this works because there's no virtual. There's no like type hierarchy at all here. And this is a bit of clever programming, and I can take no credit for this clever programming. The entire idea came from sean parent. There are all kinds of links in the actual code if you want to read more about it, but I'm going to give you the like you know, super, super fast version.

So what does this whole concept model thing? All right, the idea of a concept is that it is this abstract virtual interface. We have a virtual destructor, and we have a virtual run method. We take the unit of I r as a template parameter. is this great kind of distillation of what a passes around this. We can build any past we want. And then we have a model of this particular, you know, kind of high level concept, which is a class template, right? It derives from that, and it overrides that run method. And IT delegates that run method to some concrete type. Right? IT's just a rapper, but it's a wrapper that hides a particular interface inside of a virtual interface. And it doesn't for us so that our past doesn't have to understand virtual doesn't have to have a type hierarchy. Doesn't have to deal with any of that business. All of this is completely hidden. No one has to deal with that, right? All you have to do is you have a class. If it has a run method, it is a pass. you can add it to a pass manager that makes some vague sense to people. I wanna belabor this too much. 

All right, so if we have a past manager and we can add random passes to it, the next thing we need is we need the ability to kind of you to know move between layers of the IR. We need some way of adapting between different layers of the eye are the simplest idea I could come up with for this. So you have some kind of and yes, these names are a hilarious module to function pass adapter. Um, I'm sorry if anyone here gets kind of like java throwbacks, but we have a model to function pass adapter, and it accepts it. You know it wraps a function pass, and it is a module pass. And it does the kind of obvious thing. It takes the module, and it finds every function in the module, and it runs the function pass on each function in the model. There's nothing fancy here. Like the entire code for this is actually on the screen, right? With this, we're done.

So good keynote. Ok. So it's a little bit more complicated than that. But this is the simplest baseline we could get for kind of past management. But now we need actually to work. We need to serve the needs that LLVM has, which are much more complicated. And primarily, that has to do with analysis. Everything would be very simple. If all you did was transfer my art. Unfortunately, this whole idea of analyzing the IR is complicated. All the complexity comes from here. So an analysis passes, you know, in theory, just a special kind of pass. All right. But it has a lot of very special properties, and we want to treat it from a lot of other passes.

Okay, the kind of really, really important aspects of an analysis pass. IT's got an immutable view of the I r right? IT's never mutating the I r it produces a result. It doesn't just transform the I r it can't the IRs immutable to an analysis. It produces some result, and that result can be queried.
The result may be the logic of the analysis. In many cases, we don't compute anything. What we do is we give you a result that will lazily compute what you need when you need it. Alright, log, and likes to be lazy. I'm a big fan of this. So that's the core attributes of an analysis pass.

And I want to look at a concrete example because with analysis passes if I give you these like, you know theoretical examples, IT's just going to be boring. So we're going to look at a concrete example. It turns out that this is pretty easy. Here's the dominate a tree. Now I've deleted a bunch of comments, and I believe a bunch of other stuff. But this is the core of the dominate a tree. And what you might notice here is that dominate a tree doesn't have a single thing to do with the pass in it. Nothing is going on here about a pass. All right? And dormitory is art passes dom into trees are the result of running an analysis pass. Right? They're the result.

The analysis pass looks something like this. You have a dominate tree result. And we need a little bit more machinery to define analysis passing the new infrastructure. The first piece of machinery is that we need some way to identify analysis passes because we're going to want to run them automatically. We're going to want to manage dependencies between them. We do a lot of things with analysis passes that we don't do with transformation passes and to do that we need to have some right way of identifying them. And we do that with this kind of abstract idea that returns a void star that's unique to the past. This is pretty much the same trick the current past manager uses, and the piles of other parts of l t mean nothing too crazy. The rest of it though looks a lot more like the passes that I've been showing it. Right? There's a constructor, and it sets things up. It turns out this one's easy to set up, right? There's a run function, and it runs over some IR I've even got a bug in my slide. That's supposed to be a con structure. Nba.you know, that's fine. And it computes a result that it returns it. All the fun stuff is actually in the result. And this tends to be a pretty common pattern. In LLVM the actual analysis passes are relatively uninteresting. Most of the interesting parts are the result, but it's a pretty nice way to kind of think about how you take a piece of IR and you get this result that tells you something at a very high level about that.

All right. So the hard part of this then, like, you know, we have results already. The analysis pass itself is super, super simple. The hard part is, well, when do we run the analysis? Pass? This is actually what the question is? Ok, so historically, we had an extraordinary approach to this. In my mind, we tried to solve this as a scheduling problem. Now you may know from your computer science classes that scheduling problems are hard. They have like special classifications of how hard they can be. And it turns out that log n scheduling problem is no different. If you go on your profile, um, a debug build, and you run all the regression test a new profile, the regression test, you will find that about like ten percent of the entire time of the regression test suite is spent scheduling analysis passes, which blows my mind. We're spending all of our time figuring out in which order to run the analysis passes to satisfy their dependence is it's wasteful. And IT's completely unnecessary because we can use much simpler ways to do this by catching.

So I want to think of the entire thing as a caching problem. The way is better at solving that. IT's super easy to think about. And IT also opens the door to doing lots of more interesting things about picking women where we run analysis passes. So we pick an analysis schedule right by catching the results of lazy runs and memorizing them and just producing them later. Right? Analysis past can't you take the IR, so it doesn't matter if some other pass runs in between. When the analysis pass, you want it runs, and when your analysis pass runs. There are no constraints to solve here. It's fine just to take whatever top logical order you have, right? You cache the results you produce the cache results at each step when you need to ensure that the analyses only run once. And you've picked a perfectly good schedule.

So this is what a cashing based analysis manager might look like for functions. Ok. So we first have to have another one of these concepts and pass. We have a rule, but we also have a result concept in the past concept that is distinct. This allows us to you know actually talk about the idea of running a pass to get a result and then holding onto a result. The result has no interesting interface that we care about. The idea is to cash it somewhere. A void star would be enough, but there's a little bit of fun stuff in there. The interface for the analysis manager is very, very simple. You can get a result. If you're getting a result then maybe it will lazily run it. If it doesn't have one if it already has one it will directly provide it. You can also do get cash result. This has the kind of public property of being constants, never going to do anything. It's not going to change anything. It's just going to hand you whatever it happens to have. If it doesn't have anything that's gonna hit. You know, right, you also have the first sign of registration. You can register, and analysis passes with an analysis manager. This makes a certain amount of sense because we want to kind of know the total set of analyses that this manager is dealing within its life. Right? We also want to have the particular ability to set up an analysis past with some initial state, you know, parameters tunable, whatever they may be, and plug those into the manager so that whenever it gets queried, the query path doesn't have to know how to set up the analysis. Right? The registration took care of that for it. And the final thing, and this should kind of start to give away the ghost here is invalidate.

Because naturally, the problem immediately becomes an invalidation problem of cache invalidation problem, because this is computer science, right? And so we have to develop the cache invalidation. So the question is, uh, you know, at what point, right? Like, if you run a pass to transform the I r you have to somehow go and invalidate all the cache results, which are no longer usable. And that turns out to be where most of the complexity is. Ok. So how do we do cache invalidation of analyses? Well, the first thing we need to be able to talk about is kind of, you know what are we even thinking about it in terms of analysis? what are we trying to preserve? We need some way of actually naming a set of analyses that we care about preserving or not preserving. And so the new pass manager infrastructure provides you a pretty straightforward set class. It's got some convenient methods. It's a set of preserved analyses can get say to preserve any you can say you preserve all you can you know um mark a set as preserving something right? You can query um actually deleted one method in here that's kind of important. You can also intersect two sets, and that lets you figure out. Ok, so now I can talk about the sets of analyses I care. I'm trying to preserve, or I mean preserved or any of the other queries we might have. We end up using this in all the APIs.

So now we have to start complicating our beautiful, beautiful, simple interfaces. The first thing to realize, uh, remember, a function pass manager is a pass. So the first thing we realize is that our past interface has to get a little bit more complicated. Now when we run a pass, it has to return something. I can't return void. The specific thing in its return is what set of analyses are preserved after that past has run right? That's the first complication to the interface the next. And you can see how this is going to play together. Right? And the next thing we do is in the past manager, and we have to accumulate these things and figure out how to model the preserve set of passes. When I'm writing on aggregation. And it's not entirely surprising. What I do is I see what each of the subparts preserves. I intersect the sets. And the result of that is the set I preserve. So jumping back into this a little bit, we've got a function pass manager. The first thing we notice is that the idea of the pass has become more complicated. We're now recording what analyses are preserved. We're doing so conservatively. And we're returning that whenever we're run over a unit of I r. the next complication is that we have this analysis manager inside the past manager and in the interface to the pass. Ok. And this is important because if the analysis manager that your passes are querying isn't visible to the past manager that's running your pass, then we can't ensure that the invalidation steps occur at the right times. And we certainly can ensure they occur at the optimal times. And so we always want a pass to get its analysis manager from whatever past manager is running it, as we pass it down through the run method. Now, one thing that I couldn't find a good way to show on this slide is that this is optional. You don't have to accept and analysis manager in your past interface. If you just only have one parameter to your run function, the past manager knows how to do that. Deal with that, and it just won't pass the pass minute. The analysis manager to you. So you pay for what you use. If you don't need analyses, you don't pay for them, right? If you do, you add the parameter, and it will automatically be populated when your passes. Right. So with an analysis manager, right? All all the kind of interesting stuff that's going to happen here is that this past manager is going to first pass that analysis manager down into each of the subordinate passes. Right? And the second thing is going to invalidate any analyses that each pass fails to preserve. And so the past managers taking care of all of the cache invalidations for you, and it's using this preserved analyses set to communicate from the past, which runs to the analysis manager, what needs to be invalidated.

All right, so as we've been going through this, we're talking about analysis managers, and they all operate over units of IR just like the passes to. So the interesting thing is how do you model this when it crosses boundaries between I are um and unlike normal passes, this is a much more complex operation because this is bidirectional. All right. You can imagine a module to pass querying function analyses. You can also imagine a function of pass query module analyses. So we need to support both directions of this. Um. The other thing that's a little bit tricky is that invalidation has to be propagated by directional e write a function transformation can invalidate the entire module analysis. And a module transformation and validates every function analysis potentially. It's even trickier. How do we even identify the function for which the analysis was run if the transformation to the module removed that function? So this is kind of cache invalidation problem, where your keys may be invalidated at the same time as the values are invalidated. Ok. So it is where things started to get a little bit tricky. And you can tell, but close my slides get a bit harder to read. And I apologize. I worked hard, but it was hard. It was tough to make this fit.

So bear with me. Also, the names get silly. This is a function analysis manager, module, proxy. Okay. The ultimate idea is that this is a module analysis pass. And what it's doing is a module analysis path is absolutely nothing. You can see it's run. And all its run does is construct a result. All of the interesting logic here is in the result, and the result of this weird proxy analysis doesn't do anything either, except provide two very important methods. Oh, come to me. The first method that's super important here is that we can get the function analysis manager from this proxy. That's important. This proxy needs to be the path through which we always get that subordinate level of IR analysis manager because this proxy is responsible for ensuring and validation occurs at the appropriate points. So this is the primary interface for kind of extracting a different level of IIRs analysis manager. The second important thing is invalidation. The nice thing is that the model of a module analysis being invalidated pretty cleanly, maps on to, you know, fanning that invalidation out across all the functions within that module. Ok, so we go, and we look at what's going on. We are kind of delegate this invalidation down. Now we do something really interesting. Here. We check whether this analysis is preserved, and we only invalidate the functional analyses. If this mod proxy analysis fails to be preserved, the whole point of this is that if you have a module pass and you actually mark explicitly that you preserve this function analysis manager module proxy thing, you actually mark that you preserve this. What that says is, hey, no, I've taken care that all of the function analyses, all the functionality is that we have ever cashed are correct after whatever I've done to the module and their times. And this is pretty straightforward. For example, if you have a module path which only manipulates global, maybe it only deletes un referent to global. The function analysis passes are completely validated. You might have to do some work. You might have to go query them, but you can actually say that. And it's important that you have the ability to preserve function. Alice is when you know you didn't touch them. It also means that when the IR changed and you reserve, you return that. You know, I know I preserved everything. It actually works in this delegation layer as well.

So this handles kind of the invalidation from, you know, a higher order chunk of I r down to a lower order trunk of I r so how do we handle the reverse? Turns out the reverse is easier because, in the reverse, you have fewer choices. There is no way to lazily run a module analysis pass and produce a useful cache entry from within a function transformation. Because if you are within a function transformation pass, you're not allowed to go and look at other functions, right? You're not allowed to to to do anything with them. You can't touch them. So unless you already have some analysis cashed and ready, you you can't touch it. And so we have this lovely thing which provides uh access to the module analysis manager, but it's a Contax so it says that you can't create new cache entries. You can only query what's already in the cache. Um, and it also doesn't do anything to handle and validation because the actual past managers themselves propagate and validation back up the stack. Right? If you remember what our function pass finisher looked like, it returned the intersected set of preserves passes up to whatever is managing it. And that takes care of the invalidation for you. So this is this turns out to be the simpler of the two problems.

That's all, uh, running a little short on time. So I'm going to skip ahead. You guys had good questions. Uh, so the first thing I want to mention briefly is that we still need to understand how we funnel these analysis managers back and forth between the layers of I r and this happens inside of the adapter between the two layers of I r so this is the less simplified version of the adapter. When the adapter receives uh and analysis manager in its argument, it has to get the next level of analysis manager out of it. And then it has to pass it down, and it has to invalidate it as the thing is running. This isn't too surprising. It's just responsible for actually doing that step of delegation.

So I was going to try and go through and show you guys how to use them. But all of this code is checked in. And I've only got a few minutes. I want to skip ahead. I want to talk about a few loose ends that I haven't covered yet. Uh, automatic registration. Uh, lots of people talk about automatic registration. It's really, and it's a fundamental part of our existing stuff. And my answer is simply no. This causes endless problems for us, right? Like if you want to understand how frustrating automatic registration of passes talked either Owen or Christina men like they will tell you about the pain and suffering they have gone through trying to support this use case. Um, I don't think the use case is that important. I want passes to stop being special. I would like them to be like any other piece of code, right? We know how do you know or like orchestrate your API's so that you can you know construct an object with the various pieces of input you need and then pass it around to another object we understand how to like you know you do collect things together into containers and these types of things. These are simpler concepts. We don't need an automatic magical registry hiding behind the scenes.

Um. When it comes to things like a command line stuff we need to throw out the entire command line passing anyways because the existing command line passing magically infer structure from this like a flat sequence of arguments that don't have structure. And so there are these like even more magical arguments which kind of slightly change how that structures inferred because their special barrier passes like it's a terrible pile of hacks. Instead we could define a super simple textual syntax for specifying a pass puppy pipeline including structure and then we could pass that, and we could actually put that code in a library, and that library could be registered with the names of all the passes and then all the front ends that ever want to you know use textual ways of registering passes could use that library and there's no more need for registering command line flags for passes just to get there you know pass name in there. So that kind of obvious all these things we still need to solve something for plugins, but it seems much more straightforward to give plug into dedicated access to registering their particular name of pass rather than trying to tie it to global variables with weird initialize as really scary to me 

the other losing uh one of the other loosens I I kind of feel like I need to talk about is reuse ing pa sizz. So one thing you might notice is that you actually pass these things around by value. Okay? And so so the expectations are actually going to move the most of the time into the past managers between the past managers are going to take ownership of the things and a lot of people are worried about like well but what if that's going to increase the compilation cost? The idea here is just factor that out of the pass if you need to have a past context that maintains long live data structures have that have it outside of your pass it into the constructor of the pass right and like reference it lately there are lots of ways to actually manage data structures externally to the pass infrastructure. And this keeps writing passes very simple, which is what the majority of passes want.